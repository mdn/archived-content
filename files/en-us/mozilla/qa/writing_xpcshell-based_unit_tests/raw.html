<p>xpcshell tests are quick-to-run tests, that are generally used to write unit tests. They do not have access to the full browser chrome like <a href="/en/Browser_chrome_tests">browser chrome tests</a>, and so have much lower overhead. They are typical run by using <code>./mach xpcshell-test</code> which initiates a new <a href="/en/XPConnect/xpcshell" title="en/xpcshell">xpcshell</a> session with the xpcshell testing harness. Anything available to the XPCOM layer (through scriptable interfaces) can be tested with xpcshell. See <a href="/en/Mozilla_automated_testing" title="en/Mozilla_automated_testing">Mozilla automated testing</a> and <a class="internal" href="/docs/tag/Automated%20testing" title="Special:Tags?tag=Automated+testing">pages tagged "automated testing"</a> for more information.</p>

<h2 id="Your_first_xpcshell-based_test" name="Your_first_xpcshell-based_test">Introducing xpcshell testing</h2>

<p>xpcshell test filenames must start with <code>test_</code>.</p>

<h3 id="Creating_a_new_test_directory">Creating a new test directory</h3>

<p>If you need to create a new test directory, then follow the steps here. The test runner needs to know about the existence of the tests and how to configure them through the use of the <code>xpcshell.ini</code> manifest file.</p>

<p>First add a <code>XPCSHELL_TESTS_MANIFESTS += ['xpcshell.ini']</code> declaration (with the correct relative <code>xpcshell.ini</code> path) to the <code>moz.build</code> file located in or above the directory.</p>

<p>Then create an empty <code>xpcshell.ini</code> file to tell the build system about the individual tests, and provide any additional configuration options.</p>

<h3 id="Creating_a_new_test_in_an_existing_directory">Creating a new test in an existing directory</h3>

<p>If you're creating a new test in an existing directory, you can simply run:</p>

<pre class="brush: bash">$ ./mach addtest path/to/test/test_example.js
$ hg add path/to/test/test_example.js</pre>

<p>This will automatically create the test file and add it to <code>xpcshell.ini</code>, the second line adds it to your commit.</p>

<p>The test file contains an empty test which will give you an idea of how to write a test. There are plenty more examples throughout mozilla-central.</p>

<h3 id="Running_tests">Running tests</h3>

<p>To run the test, execute it by running the <code>mach</code> command from the root of the Gecko source code directory.</p>

<pre># Run a single test:
$ ./mach xpcshell-test path/to/tests/test_example.js

# Test an entire test suite in a folder:
$ ./mach xpcshell-test path/to/tests/

# Or run any type of test, including both xpcshell and browser chrome tests:
$ ./mach test path/to/tests/test_example.js
</pre>

<p>The test is executed by the testing harness. It will call in turn:</p>

<ul>
 <li><code>run_test</code> (if it exists).</li>
 <li>Any functions added with <code>add_task</code> or <code>add_test</code> in the order they were defined in the file.</li>
</ul>

<p>See also the notes below around <code>add_task</code> and <code>add_test</code>.</p>

<h2 id="xpcshell_test_utility_functions" name="xpcshell_test_utility_functions">xpcshell Testing API</h2>

<p>xpcshell tests have access to the following functions. They are defined in <a class="external" href="http://dxr.mozilla.org/mozilla-central/source/testing/xpcshell/head.js" title="http://mxr.mozilla.org/mozilla-central/source/testing/xpcshell/head.js">testing/xpcshell/head.js</a> and <a class="external" href="http://dxr.mozilla.org/mozilla-central/source/testing/modules/Assert.jsm" title="http://mxr.mozilla.org/mozilla-central/source/testing/modules/Assert.jsm">testing/modules/Assert.jsm</a>.</p>

<h3 id="Assertions">Assertions</h3>

<dl>
 <dt><code>Assert.ok(truthyOrFalsy[, <em>message</em>])</code></dt>
 <dt><code>Assert.equal(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.notEqual(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.deepEqual(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.notDeepEqual(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.strictEqual(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.notStrictEqual(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.rejects(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.greater(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.greaterOrEqual(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.less(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.lessOrEqual(<em>actual</em>, <em>expected</em>[, <em>message</em>])</code></dt>
 <dd>These assertion methods are provided by <a href="/en/docs/Mozilla/JavaScript_code_modules/Assert.jsm" title="/en/docs/Mozilla/JavaScript_code_modules/Assert.jsm">Assert.jsm</a>. It implements the <a href="http://wiki.commonjs.org/wiki/Unit_Testing/1.1" title="http://wiki.commonjs.org/wiki/Unit_Testing/1.1">CommonJS Unit Testing specification version 1.1</a>, which provides a basic, standardized interface for performing in-code logical assertions with optional, customizable error reporting. It is <em>highly</em> recommended to use these assertion methods, instead of the ones mentioned below. You can on all these methods remove the <code>Assert.</code> from the beginning of the name, e.g. <code>ok(true)</code> rather than <code>Assert.ok(true)</code>, however keeping the <code>Assert.</code> prefix may be seen as more descriptive and easier to spot where the tests are.</dd>
 <dt><code>Assert.throws(<em>callback</em>, <em>expectedException</em>[, <em>message</em>])</code></dt>
 <dt><code>Assert.throws(<em>callback</em>[, <em>message</em>])</code></dt>
 <dd>Asserts that the provided callback function throws an exception. The <code>expectedException</code> argument can be an <code>Error</code> instance, or a regular expression matching part of the error message (like in <code>Assert.throws(() =&gt; a.b, /is not defined/</code>).</dd>
 <dt><code>Assert.rejects(promise, expectedException[, message])</code></dt>
 <dd>Asserts that the provided promise is rejected. Note: that this should be called prefixed with an <code>await</code>. The <code>expectedException</code> argument can be an <code>Error</code> instance, or a regular expression matching part of the error message. Example: <code>await Assert.rejects(myPromise, /bad response/);</code></dd>
</dl>

<h3 id="Test_case_registration_and_execution">Test case registration and execution</h3>

<dl>
 <dt><code>add_task([<em>condition</em>, ]testFunc)</code></dt>
 <dd>Add an asynchronous function or to the list of tests that are to be run asynchronously. Whenever the function <code>await</code>s a <a href="/en-US/docs/Mozilla/JavaScript_code_modules/Promise.jsm">Promise</a>, the test runner waits until the promise is resolved or rejected before proceeding. Rejected promises are converted into exceptions, and resolved promises are converted into values.</dd>
 <dd>You can optionally specify a condition which causes the test function to be skipped; see {{anch("Adding conditions through the add_task or add_test function")}} for details.</dd>
 <dd>For tests that use <code>add_task()</code>, the <code>run_test()</code> function is optional, but if present, it should also call <code>run_next_test()</code> to start execution of all asynchronous test functions. The test cases must not call <code>run_next_test()</code>, it is called automatically when the task finishes. See {{anch("Async tests")}}, below, for more information.</dd>
 <dt><code>add_test([<em>condition</em>, ]<var>testFunction</var>)</code></dt>
 <dd>Add a test function to the list of tests that are to be run asynchronously.</dd>
 <dd>You can optionally specify a condition which causes the test function to be skipped; see {{anch("Adding conditions through the add_task or add_test function")}} for details.</dd>
 <dd>Each test function must call <code>run_next_test()</code> when it's done. For tests that use <code>add_test()</code>, <code>the run_test()</code> function is optional, but if present, it should also call <code>run_next_test()</code> to start execution of all asynchronous test functions. In most cases, you should rather use the more readable variant <code>add_task()</code>. See {{anch("Async tests")}}, below, for more information.</dd>
 <dt><code>run_next_test()</code></dt>
 <dd>Run the next test function from the list of asynchronous tests. Each test function must call <code>run_next_test()</code> when it's done. <code>run_test()</code> should also call <code>run_next_test()</code> to start execution of all asynchronous test functions. See {{anch("Async tests")}}, below, for more information.</dd>
 <dt><strong><code>registerCleanupFunction</code></strong><code>(<var>callback</var>)</code></dt>
 <dd>Executes the function <code>callback</code> after the current JS test file has finished running, regardless of whether the tests inside it pass or fail. You can use this to clean up anything that might otherwise cause problems between test runs.</dd>
 <dd>If <code>callback</code> returns a <code>Promise</code>, the test will not finish until the promise is fulfilled or rejected (making the termination function asynchronous).</dd>
 <dd>Cleanup functions are called in reverse order of registration.</dd>
 <dt><code>do_test_pending()</code></dt>
 <dd>Delay exit of the test until do_test_finished() is called. do_test_pending() may be called multiple times, and do_test_finished() must be paired with each before the unit test will exit.</dd>
 <dt><code>do_test_finished()</code></dt>
 <dd>Call this function to inform the test framework that an asynchronous operation has completed. If all asynchronous operations have completed (i.e., every do_test_pending() has been matched with a do_test_finished() in execution), then the unit test will exit.</dd>
</dl>

<h3 id="Environment">Environment</h3>

<dl>
 <dt><code>do_get_file(<var>testdirRelativePath</var>, <var>allowNonexistent</var>)</code></dt>
 <dd>Returns an {{ interface("nsILocalFile") }} object representing the given file (or directory) in the test directory. For example, if your test is unit/test_something.js, and you need to access unit/data/somefile, you would call <code>do_get_file('data/somefile')</code>. The given path must be delimited with forward slashes. You can use this to access test-specific auxiliary files if your test requires access to external files. Note that you can also use this function to get directories.
 <div class="note"><strong>Note:</strong> If your test needs access to one or more files that aren't in the test directory, you should install those files to the test directory in the Makefile where you specify <code>XPCSHELL_TESTS</code>. For an example, see {{ Source("netwerk/test/Makefile.in#117") }}.</div>
 </dd>
 <dt><code>do_get_profile()</code></dt>
 <dd>Registers a directory with the profile service and returns an {{ interface("nsILocalFile") }} object representing that directory. It also makes sure that the <strong>profile-change-net-teardown</strong>, <strong>profile-change-teardown</strong>, and <strong>profile-before-change</strong> <a href="/en/Observer_Notifications#Application_shutdown" title="en/Observer_Notifications#Application_shutdown">observer notifications</a> are sent before the test finishes. This is useful if the components loaded in the test observe them to do cleanup on shutdown (e.g., places).
 <div class="note"><strong>Note:</strong> <code>do_register_cleanup</code> will perform any cleanup operation <em>before</em> the profile and the network is shut down by the observer notifications.</div>
 </dd>
 <dt><code>do_get_idle(</code><code>)</code></dt>
 <dd>By default xpcshell tests will disable the idle service, so that idle time will always be reported as 0. Calling this function will re-enable the service and return a handle to it; the idle time will then be correctly requested to the underlying OS. The idle-daily notification could be fired when requesting idle service. It is suggested to always get the service through this method if the test has to use idle.</dd>
 <dt><code>do_get_cwd()</code></dt>
 <dd>Returns an {{ interface("nsILocalFile") }} object representing the test directory. This is the directory containing the test file when it is currently being run. Your test can write to this directory as well as read any files located alongside your test. Your test should be careful to ensure that it will not fail if a file it intends to write already exists, however.</dd>
 <dt><code>load(<var>testdirRelativePath</var>)</code></dt>
 <dd>Imports the JavaScript file referenced by <code><var>testdirRelativePath</var></code> into the global script context, executing the code inside it. The file specified is a file within the test directory. For example, if your test is unit/test_something.js and you have another file unit/extra_helpers.js, you can load the second file from the first by calling <code>load('extra_helpers.js')</code>.</dd>
</dl>

<h3 id="Utility">Utility</h3>

<dl>
 <dt><code>do_parse_document(<var>path</var>, <var>type</var>)</code></dt>
 <dd>Parses and returns a DOM document.</dd>
 <dt><code>executeSoon(<var>callback</var>)</code></dt>
 <dd>Executes the function <code>callback</code> on a later pass through the event loop. Use this when you want some code to execute after the current function has finished executing, but you don't care about a specific time delay. This function will automatically insert a <code>do_test_pending</code> / <code>do_test_finished</code> pair for you.</dd>
 <dt><code>do_timeout(<var>delay</var>, <var>fun</var>)</code></dt>
 <dd>Call this function to schedule a timeout. The given function will be called with no arguments provided after the specified delay (in milliseconds). Note that you must call <code>do_test_pending</code> so that the test isn't completed before your timer fires, and you must call <code>do_test_finished</code> when the actions you perform in the timeout complete, if you have no other functionality to test. (Note: the function argument used to be a string argument to be passed to eval, and some older branches support only a string argument or support both string and function.)</dd>
</dl>

<h3 id="Multiprocess_communication">Multiprocess communication</h3>

<dl>
 <dt><code>do_send_remote_message(name, optionalData)</code></dt>
 <dd>Asynchronously send a message to all remote processes. Pairs with <code>do_await_remote_message</code> or equivalent ProcessMessageManager listeners.</dd>
 <dt><code><strong>do_await_remote_message</strong>(name, optionalCallback)</code></dt>
 <dd>Returns a promise that is resolved when the message is received. Must be paired with<code> do_send_remote_message</code> or equivalent ProcessMessageManager calls. If <strong>optionalCallback</strong> is provided, the callback must call <code>do_test_finished</code>. If optionalData is passed to <code>do_send_remote_message</code> then that data is the first argument to <strong>optionalCallback</strong> or the value to which the promise resolves.</dd>
</dl>

<h2 id="xpcshell.ini_manifest">xpcshell.ini manifest</h2>

<p>The manifest controls what tests are included in a test suite, and the configuration of the tests. It is loaded via the `moz.build` property configuration proprety.</p>

<p>The following are all of the configuration options for a test suite as listed under the <code>[DEFAULT]</code> section of the manifest.</p>

<dl>
 <dt><code>tags</code></dt>
 <dd>Tests can be filtered by tags when running multiple tests. The command for mach is <code>./mach xpcshell-test --tag TAGNAME</code></dd>
 <dt><code>head</code></dt>
 <dd>The relative path to the head JavaScript file, which is run once before a test suite is run. The variables declared in the root scope are available as globals in the test files. See {{anch("Test head and support files")}} for more information and usage.</dd>
 <dt><code>firefox-appdir</code></dt>
 <dd>Set this to "browser" if your tests need access to things in the browser/ directory (e.g. additional XPCOM services that live there)</dd>
 <dt><code>skip-if</code></dt>
 <dt><code>run-if</code></dt>
 <dt><code>fail-if</code></dt>
 <dd>For this entire test suite, run the tests only if they meet certain conditions. See {{anch("Adding conditions in the xpcshell.ini manifest")}} for how to use these properties.</dd>
 <dt><code>support-files</code></dt>
 <dd>Make files available via the <code>resource://test/[filename]</code> path to the tests. The path can be relative to other directories, but it will be served only with the filename. See {{anch("Test head and support files")}} for more information and usage.</dd>
 <dt><code>[test_*]</code></dt>
 <dd>Test file names must start with <code>test_</code> and are listed in square brackets</dd>
</dl>

<h3 id="Creating_a_new_xpcshell.ini_file">Creating a new xpcshell.ini file</h3>

<p>When creating a new directory and new xpcshell.ini manifest file, the following must be added to a moz.build file near that file in the directory hierarchy:</p>

<pre>XPCSHELL_TESTS_MANIFESTS += ['path/to/xpcshell.ini']</pre>

<p>Typically, the moz.build containing <em>XPCSHELL_TESTS_MANIFESTS </em>is not in the same directory as <em>xpcshell.ini</em>, but rather in a parent directory. Common directory structures look like:</p>

<pre>feature
├──moz.build
└──tests/xpcshell
   └──xpcshell.ini

# or

feature
├──moz.build
└──tests
   ├──moz.build
   └──xpcshell
      └──xpcshell.ini
</pre>

<h2 id="Test_head_and_support_files" name="Test_head_and_support_files">Test head and support files</h2>

<p>Typically in a test suite, similar setup code and dependencies will need to be loaded in across each test. This can be done through the test head, which is the file declared in the <code>xpcshell.ini</code> manifest file under the <code>head</code> property. The file itself is typically called <code>head.js</code>. Any variable declared in the test head will be in the global scope of each test in that test suite.</p>

<p>In addition to the test head, other support files can be declared in the <code>xpcshell.ini</code> manifest file. This is done through the <code>support-files</code> declaration. These files will be made available through the url <code><a class="external" rel="freelink">resource://test</a></code> plus the name of the file. These files can then be loaded in using the <a href="/en/Components.utils.import"><code>Components.utils.import</code></a> function or other loaders. The support files can be located in other directory as well, and they will be made available by their filename.</p>

<pre># File structure:

path/to/tests
├──head.js
├──module.jsm
├──moz.build
├──test_example.js
└──xpcshell.ini
</pre>

<pre># xpcshell.ini
[DEFAULT]
head = head.js
support-files =
  ./module.jsm
  ../../some/other/file.js
[test_component_state.js]
</pre>

<pre class="brush: js">// head.js
var globalValue = "A global value.";

// Import support-files.
Components.utils.import("resource://test/module.jsm");
Components.utils.import("resource://test/file.jsm");
</pre>

<pre class="brush: js">// test_example.js
function run_test() {
  equal(globalValue, "A global value.", "Declarations in head.js can be accessed");
}
</pre>

<h2 id="Additional_testing_considerations">Additional testing considerations</h2>

<h3 id="Async_tests">Async tests</h3>

<p>Asynchronous tests (that is, those whose success cannot be determined until after <code>run_test</code> finishes) can be written in a variety of ways.</p>

<h4 id="Task-based_asynchronous_tests">Task-based asynchronous tests</h4>

<p>The easiest is using the <code>add_task</code> helper. <code>add_task</code> can take an asynchronous function as a parameter. <code>add_task</code> tests are run automatically if you don't have a <code>run_test</code> function.</p>

<pre class="brush: js">add_task(async function test_foo() {
  let foo = await makeFoo(); // makeFoo() returns a Promise&lt;foo&gt;
  equal(foo, expectedFoo, "Should have received the expected object");
});

add_task(async function test_bar() {
  let foo = await makeBar(); // makeBar() returns a Promise&lt;bar&gt;
  Assert.equal(bar, expectedBar, "Should have received the expected object");
});
</pre>

<h4 id="Callback-based_asynchronous_tests">Callback-based asynchronous tests</h4>

<p>You can also use <code>add_test</code>, which takes a function and adds it to the list of asynchronously-run functions. Each function given to <code>add_test</code> must also call <code>run_next_test</code> at its end. You should normally use <code>add_task</code> instead of <code>add_test</code>, but you may see <code>add_test</code> in existing tests.</p>

<pre class="brush: js">add_test(function test_foo() {
  makeFoo(function callback(foo) { // makeFoo invokes a callback&lt;foo&gt; once completed
    equal(foo, expectedFoo);
    run_next_test();
  });
});

add_test(function test_bar() {
  makeBar(function callback(bar) {
    equal(bar, expectedBar);
    run_next_test();
  });
});
</pre>

<h4 id="Other_tests">Other tests</h4>

<p>We can also tell the test harness not to kill the test process once <code>run_test()</code> is finished, but to keep spinning the event loop until our callbacks have been called and our test has completed. Newer tests prefer the use of <code>add_task</code> rather than this method. This can be achieved with <code>do_test_pending()</code> and <code>do_test_finished()</code>:</p>

<pre class="brush: js">function run_test() {
  // Tell the harness to keep spinning the event loop at least
  // until the next do_test_finished() call.
  do_test_pending();

  someAsyncProcess(function callback(result) {
    equal(result, expectedResult);

    // Close previous do_test_pending() call.
    do_test_finished();
  });
}</pre>

<h3 id="Testing_in_child_processeses">Testing in child processeses</h3>

<p>By default xpcshell tests run in the parent process. If you wish to run test logic in the child, you have several ways to do it:</p>

<ol>
 <li>Create a regular test_foo.js test, and then write a wrapper test_foo_wrap.js file that uses the <code>run_test_in_child()</code> function to run an entire script file in the child. This is an easy way to arrange for a test to be run twice, once in chrome and then later (via the _wrap.js file) in content. See /network/test/unit_ipc for examples. The <code>run_test_in_child()</code> function takes a callback, so you should be able to call it multiple times with different files, if that's useful.</li>
 <li>For tests that need to run logic in both the parent + child processes during a single test run, you may use the poorly documented <code>sendCommand()</code> function, which takes a code string to be executed on the child, and a callback function to be run on the parent when it has completed. You will want to first call do_load_child_test_harness() to set up a reasonable test environment on the child. <code>sendCommand</code> returns immediately, so you will generally want to use <code>do_test_pending</code>/<code>do_test_finished</code> with it. NOTE: this method of test has not been used much, and your level of pain may be significant. Consider option #1 if possible.</li>
</ol>

<p>See the documentation for <code>run_test_in_child()</code> and <code>do_load_child_test_harness()</code> in testing/xpcshell/head.js for more information.</p>

<h3 id="Platform-specific_tests">Platform-specific tests</h3>

<p>Sometimes you might want a test to know what platform it's running on (to test platform-specific features, or allow different behaviors). Unit tests are not normally invoked from a Makefile (unlike Mochitests), or preprocessed (so not #ifdefs), so platform detection with those methods isn't trivial.</p>

<h4 id="Runtime_detection">Runtime detection</h4>

<p>Some tests will want to only execute certain portions on specific platforms. Use <a href="https://searchfox.org/mozilla-central/rev/a0333927deabfe980094a14d0549b589f34cbe49/toolkit/modules/AppConstants.jsm#148">AppConstants.jsm</a> for determing the platform, for example:</p>

<pre class="brush: js">ChromeUtils.import("resource://gre/modules/AppConstants.jsm");

let isMac = AppConstants.platform == "macosx";
</pre>

<h3 id="Conditionally_running_a_test">Conditionally running a test</h3>

<p>There are two different ways to conditional skip a test, either through</p>

<h4 id="Adding_conditions_through_the_add_task_or_add_test_function">Adding conditions through the <code>add_task</code> or <code>add_test</code> function</h4>

<p>You can use conditionals on individual test functions instead of entire files. The condition is provided as an optional first parameter passed into <code>add_task()</code> or <code>add_test()</code>. The condition is an object which contains a function named <code>skip_if()</code>, which is an <a href="/en-US/docs/Web/JavaScript/Reference/Functions/Arrow_functions">arrow function</a> returning a boolean value which is <strong><code>true</code></strong> if the test should be skipped.</p>

<p>For example, you can provide a test which only runs on Mac OS X like this:</p>

<pre>ChromeUtils.import("resource://gre/modules/AppConstants.jsm");

add_task({
  skip_if: () =&gt; AppConstants.platform != "mac"
}, async function some_test() {
  // Test code goes here
});
</pre>

<p>Since <code>AppConstants.platform != "mac"</code> is <code>true</code> only when testing on Mac OS X, the test will be skipped on all other platforms.</p>

<div class="note">
<p><strong>Note:</strong> Arrow functions are ideal here because if your condition compares constants, it will already have been evaluated before the test is even run, meaning your output will not be able to show the specifics of what the condition is.</p>
</div>

<h4 id="Adding_conditions_in_the_xpcshell.ini_manifest">Adding conditions in the xpcshell.ini manifest</h4>

<p>Sometimes you may want to add conditions to specify that a test should be skipped in certain configurations, or that a test is known to fail on certain platforms. You can do this in xpcshell manifests by adding annotations below the test file entry in the manifest, for example:</p>

<pre>[test_example.js]
skip-if = os == 'win'
</pre>

<p>This example would skip running <code>test_example.js</code> on Windows.</p>

<div class="note">
<p><strong>Note:</strong> Starting with Gecko {{geckoRelease(40)}}, you can use conditionals on individual test functions instead of on entire files. See {{anch("Adding conditions through the add_task or add_test function")}} above for details.</p>
</div>

<p>There are currently four conditionals you can specify:</p>

<h5 id="skip-if">skip-if</h5>

<p><code>skip-if</code> tells the harness to skip running this test if the condition evaluates to true. You should use this only if the test has no meaning on a certain platform, or causes undue problems like hanging the test suite for a long time.</p>

<h5 id="run-if">run-if</h5>

<p><code>run-if</code> tells the harness to only run this test if the condition evaluates to true. It functions as the inverse of <code>skip-if</code>.</p>

<h5 id="fail-if">fail-if</h5>

<p><code>fail-if</code> tells the harness that this test is expected to fail if the condition is true. If you add this to a test, make sure you file a bug on the failure and include the bug number in a comment in the manifest, like:</p>

<pre>[test_example.js]
# bug xxxxxx
fail-if = os == 'linux'</pre>

<h5 id="run-sequentially">run-sequentially</h5>

<p><code>run-sequentially </code>basically tells the harness to run the respective test in isolation. This is required for tests that are not "thread-safe". You should do all you can to avoid using this option, since this will kill performance. However, we understand that there are some cases where this is imperative, so we made this option available. If you add this to a test, make sure you specify a reason and possibly even a bug number, like:</p>

<pre>[test_example.js]
run-sequentially = Has to launch Firefox binary, bug 123456.</pre>

<h5 id="Manifest_conditional_expressions">Manifest conditional expressions</h5>

<p>For a more detailed description of the syntax of the conditional expressions, as well as what variables are available, <a href="/en/XPCshell_Test_Manifest_Expressions" title="en/XPCshell Test Manifest Expressions">see this page</a>.</p>

<h5 id="Running_a_specific_test_only">Running a specific test only</h5>

<p>When working on a specific feature or issue, it is convenient to only run a specific task from a whole test suite. Use <code>.only()</code> for that purpose:</p>

<pre class="syntaxbox">add_task(async function some_test() {
  // Some test.
});

add_task(async function some_interesting_test() {
// Only this test will be executed.
}).only();
</pre>

<h3 id="Problems_with_pending_events_and_shutdown">Problems with pending events and shutdown</h3>

<p>Events are not processed during test execution if not explicitly triggered. This sometimes causes issues during shutdown, when code is run that expects previously created events to have been already processed. In such cases, this code at the end of a test can help:</p>

<pre>let thread = gThreadManager.currentThread;
while (thread.hasPendingEvents())
  thread.processNextEvent(true);</pre>

<h2 id="Debugging_xpcshell-tests" name="Debugging_xpcshell-tests">Debugging xpcshell-tests</h2>

<h3 id="Running_unit_tests_under_the_javascript_debugger">Running unit tests under the javascript debugger</h3>

<h4 id="Via_--jsdebugger">Via --jsdebugger</h4>

<p>You can specify flags when issuing the <code>xpcshell-test</code> command that will cause your test to stop right before running so you can attach the <a href="/docs/Tools/Tools_Toolbox">javascript debugger</a>.</p>

<p>Example:</p>

<pre class="bash">$ ./mach xpcshell-test --jsdebugger browser/components/tests/unit/test_browserGlue_pingcentre.js
 0:00.50 INFO Running tests sequentially.
...
 0:00.68 INFO ""
 0:00.68 INFO "*******************************************************************"
 0:00.68 INFO "Waiting for the debugger to connect on port 6000"
 0:00.68 INFO ""
 0:00.68 INFO "To connect the debugger, open a Firefox instance, select 'Connect'"
 0:00.68 INFO "from the Developer menu and specify the port as 6000"
 0:00.68 INFO "*******************************************************************"
 0:00.68 INFO ""
 0:00.71 INFO "Still waiting for debugger to connect..."
...
</pre>

<p>At this stage in a running Firefox instance:</p>

<ul>
 <li>Go to the three-bar menu, then select <code>Web Developer</code> -&gt; <code>Remote Debugging</code></li>
 <li>A new tab is opened. In the Network Location box, enter <code>localhost:6000</code> and select <code>Connect</code></li>
 <li>You should then get a link to <em><code>Main Process</code></em>, click it and the Developer Tools debugger window will open.</li>
 <li>It will be paused at the start of the test, so you can add breakpoints, or start running as appropriate.</li>
</ul>

<p>If you get a message such as:</p>

<pre> 0:00.62 ERROR Failed to initialize debugging: Error: resource://devtools appears to be inaccessible from the xpcshell environment.
This can usually be resolved by adding:
  firefox-appdir = browser
to the xpcshell.ini manifest.
It is possible for this to alter test behevior by triggering additional browser code to run, so check test behavior after making this change.</pre>

<p>This is typically a test in core code. You can attempt to add that to the xpcshell.ini, however as it says, it might affect how the test runs and cause failures. Generally the firefox-appdir should only be left in xpcshell.ini for tests that are in the browser/ directory, or are Firefox-only.</p>

<h3 id="Running_unit_tests_under_a_C.2B.2B_debugger" name="Running_unit_tests_under_a_C.2B.2B_debugger">Running unit tests under a C++ debugger</h3>

<h4 id="Via_check-interactive" name="Via_check-interactive">Via <code>--debugger and -debugger-interactive</code></h4>

<p>You can specify flags when issuing the <code>xpcshell-test</code> command that will launch xpcshell in the specified debugger (implemented in {{ Bug(382682) }}). Provide the full path to the debugger, or ensure that the named debugger is in your system PATH.</p>

<p>Example:</p>

<pre class="eval">$ ./mach xpcshell-test --debugger gdb --debugger-interactive netwerk/test/unit/test_resumable_channel.js
# js&gt;_execute_test();
...failure or success messages are printed to the console...
# js&gt;quit();
</pre>

<p>On Windows with the VS debugger:</p>

<pre class="eval">$ ./mach xpcshell-test --debugger devenv --debugger-interactive netwerk/test/test_resumable_channel.js
</pre>

<p>Or with WinDBG:</p>

<pre class="eval">$ ./mach xpcshell-test --debugger windbg --debugger-interactive netwerk/test/test_resumable_channel.js
</pre>

<p>Or with modern WinDbg (WinDbg Preview as of April 2020):</p>

<pre class="eval">$ ./mach xpcshell-test --debugger WinDbgX --debugger-interactive netwerk/test/test_resumable_channel.js
</pre>

<h3 id="Debugging_xpcshell_tests_in_a_child_process">Debugging xpcshell tests in a child process</h3>

<p>To debug the child process, where code is often being run in a project, set MOZ_DEBUG_CHILD_PROCESS=1 in your environment (or on the command line) and run the test. You will see the child process emit a printf with its process ID, then sleep. Attach a debugger to the child's pid, and when it wakes up you can debug it:</p>

<pre>$ MOZ_DEBUG_CHILD_PROCESS=1 ./mach xpcshell-test test_simple_wrap.js
CHILDCHILDCHILDCHILD
  debug me @13476</pre>

<h4 id="Debug_both_parent_and_child_processes">Debug both parent and child processes</h4>

<p>Use MOZ_DEBUG_CHILD_PROCESS=1 to attach debuggers to each process. (For gdb at least, this means running separate copies of gdb, one for each process.)</p>